{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85aa1d69",
   "metadata": {},
   "source": [
    "### 【 D0127_Mini-Project_이준기 】\n",
    "\n",
    "- 민원 처리 (NLP)\n",
    "- 텍스트 -> Task 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acee74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 딥러닝 모듈 로딩\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from   torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from   torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from   torch.utils.data import random_split\n",
    "from   sklearn.model_selection import train_test_split\n",
    "\n",
    "import util_func as uf\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35ef23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 준비\n",
    "DATA_FILE = '../Project/PrData/train.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "df.nunique()\n",
    "\n",
    "NUM_CLASSES = df['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d615bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전처리 함수 생성\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "stopwords = ['합니다', '바랍니다', '부탁', '요청', '제발', '주세요', '하십시오']\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = re.sub('[^가-힣 ]', ' ', text) # 한글 + 공백만 남기기\n",
    "\n",
    "    nouns = okt.nouns(text)\n",
    "\n",
    "    nouns = [word for word in nouns if word not in stopwords and len(word) > 1]\n",
    "\n",
    "    return ' '.join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2404951",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전처리 잘 되는지 임시 텍스트로 테스트해보기\n",
    "# text = \"초등학교 바로 근처에 피부샵 간판에 '브라질리언'이라고 써있으니 초등생 아이가 그게 뭐냐고 물어봅니다. 선뜻 대답하기 어려웠고 요즘 아이들은 거의 다 스마트폰이 있으니 검색해 보고 음란 사진에 접할까 봐 염려되니 제재 바랍니다.\"\n",
    "\n",
    "# text = preprocessing(text)\n",
    "\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08de6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전처리 적용\n",
    "df['clean_text'] = df['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd14e8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>건축허가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>건축허가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>건축허가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>건축허가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>건축허가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>환경미화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>환경미화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799997</th>\n",
       "      <td>환경미화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>환경미화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>환경미화</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0       건축허가\n",
       "1       건축허가\n",
       "2       건축허가\n",
       "3       건축허가\n",
       "4       건축허가\n",
       "...      ...\n",
       "799995  환경미화\n",
       "799996  환경미화\n",
       "799997  환경미화\n",
       "799998  환경미화\n",
       "799999  환경미화\n",
       "\n",
       "[800000 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s = df.iloc[:, 1:]\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e186b76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a4f7762",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'clean_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDT\\anaconda3\\envs\\NLP\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'clean_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## ----------------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m## 비율 확인\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m## ----------------------------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdf_s\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclean_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts(normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDT\\anaconda3\\envs\\NLP\\Lib\\site-packages\\pandas\\core\\frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KDT\\anaconda3\\envs\\NLP\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'clean_text'"
     ]
    }
   ],
   "source": [
    "## ----------------------------------------------------------\n",
    "## 비율 확인\n",
    "## ----------------------------------------------------------\n",
    "df_s['clean_text'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee70663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "## 타겟 컬럼 라벨 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_s['label_idx'] = le.fit_transform(df_s['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for text in df_s['clean_text']:\n",
    "    counter.update(text.split())\n",
    "\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common())}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"VOCAB SIZE:\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e90b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "\n",
    "def text_to_seq(text):\n",
    "    return [vocab.get(word, 1) for word in text.split()]\n",
    "\n",
    "def pad_seq(seq, max_len=MAX_LEN):\n",
    "    return seq[:max_len] + [0] * (max_len - len(seq))\n",
    "\n",
    "df_s['seq'] = df_s['clean_text'].apply(text_to_seq)\n",
    "df_s['seq_pad'] = df_s['seq'].apply(pad_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293abdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------\n",
    "## 데이터 나누기\n",
    "## ----------------------------------------------------------\n",
    "featureDF = pd.DataFrame(df_s['seq_pad'].to_list())\n",
    "targetDF  = df_s[['label_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6098cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BS      = 64\n",
    "EPOCHS = 10\n",
    "LR     = 1e-3\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.xTS = torch.tensor(featureDF.values, dtype=torch.long)\n",
    "        self.yTS = torch.tensor(targetDF.values.squeeze(), dtype=torch.long)\n",
    "        self.length = featureDF.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xTS[idx], self.yTS[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    featureDF,\n",
    "    targetDF,\n",
    "    test_size=0.2,\n",
    "    random_state=10,\n",
    "    stratify=targetDF\n",
    ")\n",
    "\n",
    "trainDS = TextDataset(x_train, y_train)\n",
    "validDS = TextDataset(x_valid, y_valid)\n",
    "\n",
    "trainDL = DataLoader(trainDS, batch_size=BS, shuffle=True)\n",
    "validDL = DataLoader(validDS, batch_size=BS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)     # (B, T, E)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        out = self.fc(h[-1])      # (B, C)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_f1\": [],\n",
    "    \"val_f1\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_F1 = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # ======================\n",
    "    # Train\n",
    "    # ======================\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y in trainDL:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    train_loss /= len(trainDL.dataset)\n",
    "    train_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    # ======================\n",
    "    # Validation\n",
    "    # ======================\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in validDL:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            valid_loss += loss.item() * x.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    valid_loss /= len(validDL.dataset)\n",
    "    valid_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(valid_loss)\n",
    "    history[\"train_f1\"].append(train_f1)\n",
    "    history[\"val_f1\"].append(valid_f1)\n",
    "\n",
    "    print(\n",
    "        f\"[{epoch+1:02d}] \"\n",
    "        f\"LOSS train/valid = {train_loss:.4f} / {valid_loss:.4f} | \"\n",
    "        f\"F1 train/valid = {train_f1:.4f} / {valid_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    if valid_f1 > BEST_F1:\n",
    "        BEST_F1 = valid_f1\n",
    "        torch.save(model.state_dict(), \"best_text_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
