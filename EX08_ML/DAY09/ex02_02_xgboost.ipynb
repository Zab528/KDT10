{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdd0eb4",
   "metadata": {},
   "source": [
    "#### 【 앙상블 – GBM (Gradient Boosting Machine) 】\n",
    "\n",
    "- 이전 모델의 예측 오차를 손실함수의 기울기(gradient)로 계산 -> 다음 모델 학습하는 알고리즘\n",
    "- 손실함수 최소화하는 경사하강법 -> 각 단계에서 손실함수 기울기(잔차) 예측하는 결정트리 하나씩 추가\n",
    "- 회귀 / 분류 모두 사용 가능\n",
    "- 미분 가능한 손실함수를 단계적으로 최소화하는 알고리즘 => 예: MSE, Log-loss, Huber loss 등\n",
    "\n",
    "- 각 단계에서 손실함수의 음의 기울기(잔차, residual)를 새로운 모델이 학습\n",
    "    * 예측이 많이 틀린 샘플 → 큰 잔차(gradient)\n",
    "    * 예측이 적게 틀린 샘플 → 작은 잔차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2e68d",
   "metadata": {},
   "source": [
    "[1] Gradient Boosting = 손실함수 기울기(=잔차) 타깃으로 트리 하나씩 더해가는 과정<hr>\n",
    "\n",
    "- loss를 예측값에 대해 미분해서 나온 gradient를 트리가 근사\n",
    "- 예시 \n",
    "    * 구성 : 회귀 + MSE 손실\n",
    "    * gradient : (y - y_pred) 형태의 잔차로 떨어져 직관이 가장 \n",
    "    * 트리 : 미분이 아니라 “잔차를 맞추는 회귀” 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877fa0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Gradient Boosting 원리 데모 (MSE) ==\n",
      "F0 (상수 예측) = mean(y) = 3.0388\n",
      "[iter 0] MSE = 7.881530\n",
      "\n",
      "[iter 1]\n",
      "  residual(처음 5개) = [5.98658138 5.28761303 5.03587779 4.83445236 3.86868435]\n",
      "  tree_update(처음 5개) = [3.40571662 3.40571662 3.40571662 3.40571662 3.40571662]\n",
      "  MSE = 4.644033\n",
      "\n",
      "[iter 2]\n",
      "  residual(처음 5개) = [4.96486639 4.26589804 4.0141628  3.81273738 2.84696936]\n",
      "  tree_update(처음 5개) = [1.69176059 1.69176059 1.69176059 1.69176059 1.69176059]\n",
      "  MSE = 2.794287\n",
      "\n",
      "[iter 3]\n",
      "  residual(처음 5개) = [4.45733821 3.75836986 3.50663462 3.3052092  2.33944118]\n",
      "  tree_update(처음 5개) = [3.75688797 3.75688797 3.75688797 3.75688797 1.95952193]\n",
      "  MSE = 1.774396\n",
      "\n",
      "[iter 4] MSE = 1.206327\n",
      "[iter 5] MSE = 0.786906\n",
      "[iter 6] MSE = 0.510643\n",
      "[iter 7] MSE = 0.359739\n",
      "[iter 8] MSE = 0.265214\n",
      "[iter 9] MSE = 0.181347\n",
      "[iter 10] MSE = 0.142557\n",
      "\n",
      "핵심 요약:\n",
      " - 트리를 미분한 게 아니라, Loss를 y_pred에 대해 미분(grad)해서 residual을 만들었고\n",
      " - 트리는 그 residual을 '회귀 타깃'으로 근사했으며\n",
      " - 모델은 F <- F + lr * tree_update 로 누적 갱신됩니다.\n"
     ]
    }
   ],
   "source": [
    "## ==================================================\n",
    "## 모듈 로딩\n",
    "## ==================================================\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## 재현성 설정\n",
    "np.random.seed(42)\n",
    "\n",
    "## ==================================================\n",
    "## 1) 장난감 데이터(비선형) 생성\n",
    "## ==================================================\n",
    "n = 80\n",
    "X = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = (X[:, 0] ** 2) + 0.3 * np.sin(3 * X[:, 0]) + np.random.normal(0, 0.3, size=n)\n",
    "\n",
    "## ==================================================\n",
    "## 2) \"미니 Gradient Boosting\" 구현 (MSE 기준)\n",
    "##    - F0(x) = y의 평균 (상수 모델)\n",
    "##    - 매 반복마다:\n",
    "##        (a) 현재 예측 y_pred = F_{m-1}(x)\n",
    "##        (b) loss의 기울기: dL/dy_pred = (y_pred - y)\n",
    "##        (c) 음의 기울기(=pseudo-residual): r = -(dL/dy_pred) = (y - y_pred)\n",
    "##        (d) 트리 h_m(x)를 r를 타깃으로 학습\n",
    "##        (e) F_m(x) = F_{m-1}(x) + lr * h_m(x)\n",
    "## ==================================================\n",
    "M = 10\n",
    "lr = 0.3\n",
    "\n",
    "## 초기 모델: 상수 (y 평균)\n",
    "F0 = np.mean(y)\n",
    "y_pred = np.full_like(y, F0, dtype=float)\n",
    "\n",
    "trees = []\n",
    "\n",
    "print(\"== Gradient Boosting 원리 데모 (MSE) ==\")\n",
    "print(f\"F0 (상수 예측) = mean(y) = {F0:.4f}\")\n",
    "print(f\"[iter 0] MSE = {mean_squared_error(y, y_pred):.6f}\\n\")\n",
    "\n",
    "for m in range(1, M + 1):\n",
    "    # (b) MSE Loss: L = 1/2 * (y - y_pred)^2 라고 두면\n",
    "    #     dL/dy_pred = (y_pred - y)\n",
    "    grad = (y_pred - y)\n",
    "\n",
    "    # (c) 음의 기울기(= pseudo-residual)\n",
    "    residual = -grad  # == (y - y_pred)\n",
    "\n",
    "    # (d) residual을 예측하는 약한 학습기(얕은 트리) 학습\n",
    "    tree = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "    tree.fit(X, residual)\n",
    "    trees.append(tree)\n",
    "\n",
    "    # (e) 모델 업데이트\n",
    "    update = tree.predict(X)\n",
    "    y_pred = y_pred + lr * update\n",
    "\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "    # 학습 과정 출력(일부 값만)\n",
    "    if m <= 3:\n",
    "        print(f\"[iter {m}]\")\n",
    "        print(f\"  residual(처음 5개) = {residual[:5]}\")\n",
    "        print(f\"  tree_update(처음 5개) = {update[:5]}\")\n",
    "        print(f\"  MSE = {mse:.6f}\\n\")\n",
    "    else:\n",
    "        print(f\"[iter {m}] MSE = {mse:.6f}\")\n",
    "\n",
    "print(\"\\n핵심 요약:\")\n",
    "print(\" - 트리를 미분한 게 아니라, Loss를 y_pred에 대해 미분(grad)해서 residual을 만들었고\")\n",
    "print(\" - 트리는 그 residual을 '회귀 타깃'으로 근사했으며\")\n",
    "print(\" - 모델은 F <- F + lr * tree_update 로 누적 갱신됩니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c9430",
   "metadata": {},
   "source": [
    "[2] 회귀 예제 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bd724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1344.70\n"
     ]
    }
   ],
   "source": [
    "## 모듈 로딩\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## 1. 데이터 생성\n",
    "X, y = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    noise=10.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "## 2. 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "## 3. 모델 생성 (CPU)\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    tree_method=\"hist\",   # CPU\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "## 4. 학습\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "## 5. 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
